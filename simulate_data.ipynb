{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from scipy.stats import lognorm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../../../analyzing_serverless_in_the_wild')\n",
    "\n",
    "import source.utilities as utilities \n",
    "import source.policy as policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAIN\n",
    "def train(data, config):\n",
    "\n",
    "    apps = data['HashApp']\n",
    "\n",
    "    data = data.drop(columns=['HashApp'])\n",
    "    data = np.array(data)\n",
    "\n",
    "    result = policy.hybrid_policy(data, config)\n",
    "    \n",
    "    result = pd.DataFrame(result)\n",
    "    result['HashApp'] = apps\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "## TEST/EVAL\n",
    "def test(data, policy_result):\n",
    "\n",
    "    hist_policy = policy_result[policy_result['Policy'] == 'Histogram']\n",
    "    fixed_policy = policy_result[(policy_result['Policy'] == 'Fixed') | (policy_result['Policy'] == 'Arima')]\n",
    "    # arima_policy = policy_result[policy_result['Policy'] == 'Arima']\n",
    "\n",
    "    pre_warm = hist_policy[['HashApp','PreWarm']]\n",
    "    keep_alive = hist_policy[['HashApp','KeepAlive']]\n",
    "        \n",
    "    apps = data['HashApp']\n",
    "    data['HashApp'] = apps\n",
    "\n",
    "    hist_policy_chunk = data.merge(hist_policy['HashApp'], on='HashApp', how='inner')\n",
    "\n",
    "    pre_warm_chunk = pre_warm.merge(apps, on='HashApp', how='inner')\n",
    "    pre_warm_chunk = pre_warm_chunk['PreWarm']\n",
    "\n",
    "    keep_alive_chunk = keep_alive.merge(apps, on='HashApp', how='inner')\n",
    "    keep_alive_chunk = keep_alive_chunk['KeepAlive']\n",
    "\n",
    "    hist_policy_chunk.columns = ['HashApp'] + list(range(1, 1440*2 + 1))\n",
    "    \n",
    "    fixed_policy_chunk = data.merge(fixed_policy['HashApp'], on='HashApp', how='inner')\n",
    "    fixed_policy_chunk.columns = ['HashApp'] + list(range(1, 1440*2 + 1))\n",
    "\n",
    "    sim_result_hist = utilities.compute_simulation(hist_policy_chunk,\n",
    "                                                prewarm_window=pre_warm_chunk,\n",
    "                                                keep_alive_window=keep_alive_chunk)\n",
    "    sim_result_fixed = utilities.compute_simulation(fixed_policy_chunk,\n",
    "                                                prewarm_window=0,\n",
    "                                                keep_alive_window=10)\n",
    "    sim_result_arima = None\n",
    "    \n",
    "\n",
    "    return sim_result_hist, sim_result_fixed, sim_result_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"chunk_invocation_data\"\n",
    "path_results = \"simulation_results\"\n",
    "num_chunks = len(os.listdir(dir_path))\n",
    "chunk_range = range(1, num_chunks + 1)\n",
    "\n",
    "def train_test_hybrid_policy(config):\n",
    "    policy_result_list = []\n",
    "    unique_apps = set() # all hashes of apps that exist within the train dataset\n",
    "\n",
    "    hist_results = []\n",
    "    fixed_results = []\n",
    "    # arima_results = []\n",
    "\n",
    "\n",
    "    for chunk_id in chunk_range:\n",
    "        chunk_filename = f'chunk_{chunk_id}.csv'\n",
    "        file_path = os.path.join(dir_path, chunk_filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        policy_result = train(data.iloc[:,:-1440*2], config)\n",
    "\n",
    "        unique_apps.update(policy_result['HashApp'])\n",
    "        policy_result_list.append(policy_result)\n",
    "\n",
    "        sim_hist, sim_fixed, sim_arima = test(pd.concat([data['HashApp'],data.iloc[:,-1440*2:]], axis=1), policy_result)\n",
    "        \n",
    "        hist_results.append(sim_hist)\n",
    "        fixed_results.append(sim_fixed)\n",
    "        # arima_results.append(sim_arima)\n",
    "\n",
    "\n",
    "    policy_result = pd.concat(policy_result_list)\n",
    "    hist_results = pd.concat(hist_results)\n",
    "    fixed_results = pd.concat(fixed_results)\n",
    "    return policy_result, hist_results, fixed_results\n",
    "    # arima_results = pd.concat(arima_results)\n",
    "\n",
    "\n",
    "def test_fixed_policy(keep_alive=10):\n",
    "    fixed_results = []\n",
    "\n",
    "    for chunk_id in chunk_range:\n",
    "        chunk_filename = f'chunk_{chunk_id}.csv'\n",
    "        file_path = os.path.join(dir_path, chunk_filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "        subset = data.iloc[:, -1440*2:].copy()\n",
    "        subset.columns = list(range(1, 1440*2 + 1))\n",
    "        subset['HashApp'] = data['HashApp']\n",
    "\n",
    "        sim_result_fixed = utilities.compute_simulation(subset,\n",
    "                                                prewarm_window=0,\n",
    "                                                keep_alive_window=keep_alive)\n",
    "        \n",
    "        fixed_results.append(sim_result_fixed)\n",
    "        \n",
    "    return pd.concat(fixed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'histogram_threshold': 4,\n",
    "    'oob_threshold': 0.2,\n",
    "    'cv_threshold': 2,\n",
    "    'pctl_lower': 5,\n",
    "    'pctl_upper': 99\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYBRID PERCENTILES\n",
    "result_list = []\n",
    "for percentiles in [(0,100), (0,99), (5,100), (1,99), (5,99), (1,95), (5,95)]:\n",
    "    filename = f'Hybrid_{percentiles}.csv'\n",
    "    file_path = os.path.join(path_results, filename)\n",
    "    \n",
    "    config_perc = copy.deepcopy(config)\n",
    "    config['pctl_lower'] = percentiles[0]\n",
    "    config['pctl_upper'] = percentiles[1]\n",
    "    policy_result, hist_results, fixed_results = train_test_hybrid_policy(config)\n",
    "    sim_result = pd.concat([hist_results, fixed_results])\n",
    "    sim_result.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keep_alive in [5,10,20,30,45,60,90,120,20_000]:\n",
    "    filename = f'Fixed_{keep_alive}.csv'\n",
    "    file_path = os.path.join(path_results, filename)\n",
    "\n",
    "    sim_result = test_fixed_policy(keep_alive)\n",
    "    sim_result.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for cv in [0,2,5,10]:\n",
    "    filename = f'Hybrid_CV_{cv}.csv'\n",
    "    file_path = os.path.join(path_results, filename)\n",
    "\n",
    "    config_cv = copy.deepcopy(config)\n",
    "    config_cv['pctl_lower'] = 5\n",
    "    config_cv['pctl_upper'] = 99\n",
    "    config_cv['cv_threshold'] = cv\n",
    "    policy_result, hist_results, fixed_results = train_test_hybrid_policy(config_cv)\n",
    "    sim_result = pd.concat([hist_results, fixed_results])\n",
    "    sim_result.to_csv(file_path)\n",
    "    sim_result = sim_result[['ColdStartPercentage','WastedMemoryRatio']]\n",
    "\n",
    "    plot_dict = {}\n",
    "    plot_dict['DF'] = sim_result\n",
    "    plot_dict['Label'] = f\"CV={cv}\"\n",
    "    plot_dict['Linestyle'] = '-'\n",
    "\n",
    "    result_list.append(plot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_average(histograms):\n",
    "    bin_numbers = np.arange(1, histograms.shape[1] + 1)\n",
    "\n",
    "    weighted_values = histograms * bin_numbers\n",
    "\n",
    "    sum_weighted = weighted_values.sum(axis=1)\n",
    "\n",
    "    sum_of_bins = histograms.sum(axis=1)\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        weighted_average = sum_weighted / sum_of_bins\n",
    "        weighted_average = np.where(sum_of_bins != 0, weighted_average, 0)\n",
    "\n",
    "\n",
    "    return weighted_average\n",
    "\n",
    "w_avg_results = []\n",
    "\n",
    "for chunk_id in chunk_range:\n",
    "    chunk_filename = f'chunk_{chunk_id}.csv'\n",
    "    file_path = os.path.join(dir_path, chunk_filename)\n",
    "    data = pd.read_csv(file_path)\n",
    "    subset = data.iloc[:, -1440*2:].copy()\n",
    "    subset.columns = list(range(1, 1440*2 + 1))\n",
    "    subset['HashApp'] = data['HashApp']\n",
    "\n",
    "    subset_iit = subset.drop(columns=['HashApp'])\n",
    "    subset_iit = np.array(subset)\n",
    "\n",
    "    iit = utilities.compute_inter_invocation_times(subset_iit)\n",
    "    hist = utilities.compute_histogram(iit)\n",
    "    hist = hist[:,:240]\n",
    "    w_avg = calculate_weighted_average(hist)\n",
    "\n",
    "    keep_alive = np.round(w_avg)\n",
    "\n",
    "    sim_result_avg = utilities.compute_simulation(subset,\n",
    "                                            prewarm_window=0,\n",
    "                                            keep_alive_window=keep_alive)\n",
    "    w_avg_results.append(sim_result_avg)\n",
    "\n",
    "w_avg_results = pd.concat(w_avg_results)\n",
    "w_avg_results.to_csv('simulation_results/weighted_average.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for window in [1,2,3,4]:\n",
    "    filename = f'Hybrid_Window_{window}.csv'\n",
    "    file_path = os.path.join(path_results, filename)\n",
    "    \n",
    "    config_window = copy.deepcopy(config)\n",
    "    config_window['histogram_threshold'] = window\n",
    "    config_window['pctl_lower'] = 5\n",
    "    config_window['pctl_upper'] = 99\n",
    "    policy_result, hist_results, fixed_results = train_test_hybrid_policy(config_window)\n",
    "    sim_result = pd.concat([hist_results, fixed_results])\n",
    "    sim_result.to_csv(file_path)\n",
    "    sim_result = sim_result[['ColdStartPercentage','WastedMemoryRatio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE FOR RUNNING ARIMA, NOT USED AS TRAINING MODELS TAKES A LONG TIME\n",
    "\n",
    "# chunk_filename = 'chunk_1.csv'\n",
    "# file_path = os.path.join(dir_path, chunk_filename)\n",
    "# data = pd.read_csv(file_path)\n",
    "\n",
    "# policy_result = train(data.iloc[:,:-1440*2])\n",
    "\n",
    "# unique_apps.update(policy_result['HashApp'])\n",
    "\n",
    "# arima_policy = policy_result[policy_result['Policy'] == 'Arima']\n",
    "# arima_chunk = data.merge(arima_policy['HashApp'], on='HashApp', how='inner')\n",
    "# arima_chunk_train = arima_chunk.iloc[:,:-1440*2]\n",
    "# arima_chunk_test = arima_chunk.iloc[:,-1440*2:]\n",
    "# arima_chunk_test['HashApp'] = arima_chunk_train['HashApp']\n",
    "\n",
    "# apps = arima_chunk_train['HashApp']\n",
    "# app = apps[0]\n",
    "\n",
    "# print(app)\n",
    "# # Get training data for the app\n",
    "# train_app = arima_chunk_train[arima_chunk_train['HashApp'] == app]\n",
    "# train_app = arima_chunk_train.drop(columns=['HashApp'])\n",
    "# test_app = train_app.to_numpy().flatten()\n",
    "# test_app\n",
    "\n",
    "# # Run the function\n",
    "# cold_starts, function_durations, keep_alive_times = utilities.train_test_arima(arima_chunk_train, arima_chunk_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
